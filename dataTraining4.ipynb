{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f529d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, InputLayer, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "from supabase import create_client\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21e1b690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "804c0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "supabase = create_client(os.getenv('SUPABASE_URL'), os.getenv('SUPABASE_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "426ee8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['temp', 'humidity', 'precip', 'windspeed']\n",
    "cities = ['Caloocan', 'Las Piñas', 'Makati', 'Malabon', 'Mandaluyong', \n",
    "          'Manila', 'Marikina', 'Muntinlupa', 'Navotas', 'Parañaque',\n",
    "          'Pasay', 'Pasig', 'Quezon', 'San Juan', 'Taguig', 'Valenzuela']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06f31832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_name(city):\n",
    "    \"\"\"Convert city name to table name format\"\"\"\n",
    "    city = city.lower().replace(' ', '_').replace('ñ', 'n')\n",
    "    if city == \"las_piñas\": city = \"las_pinas\"\n",
    "    if city == \"marikina\": city = \"markina\"\n",
    "    if city == \"parañaque\": city = \"paramaque\"\n",
    "    return f\"{city}_city_weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bd7f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_city_data(city):\n",
    "    \"\"\"Fetch and preprocess city data\"\"\"\n",
    "    table_name = get_table_name(city)\n",
    "    response = supabase.table(table_name).select(\"*\").execute()\n",
    "    df = pd.DataFrame(response.data)\n",
    "    \n",
    "    # Convert and set datetime index\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    # Select only the features we need\n",
    "    df = df[features].copy()\n",
    "    \n",
    "    # Forward fill missing values\n",
    "    df = df.ffill()\n",
    "    \n",
    "    # Add simple moving average to smooth data\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].rolling(7, min_periods=1).mean()\n",
    "    \n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddd00163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window_size=60, forecast_size=7):\n",
    "    \"\"\"Create sequences ensuring homogeneous shape\"\"\"\n",
    "    X, y = [], []\n",
    "    data_values = data[features].values\n",
    "    \n",
    "    for i in range(len(data_values) - window_size - forecast_size + 1):\n",
    "        X.append(data_values[i:i + window_size])\n",
    "        y.append(data_values[i + window_size:i + window_size + forecast_size])\n",
    "    \n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8017f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \"\"\"Build LSTM model architecture\"\"\"\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape),\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(128),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(4 * 7)\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1314211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(city, df):\n",
    "    \"\"\"Train model with proper data scaling\"\"\"\n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(df), \n",
    "                             columns=df.columns, \n",
    "                             index=df.index)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(scaled_data)\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model((X.shape[1], X.shape[2]))\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5),\n",
    "        ModelCheckpoint(f'weatherModels/{city}_best_model.keras', \n",
    "                       save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, data, scaler, window_size=60, forecast_days=7):\n",
    "    \"\"\"Generate predictions with proper scaling\"\"\"\n",
    "    # Get last window of data\n",
    "    last_window = data.iloc[-window_size:].copy()\n",
    "    \n",
    "    # Scale the data\n",
    "    scaled_window = scaler.transform(last_window)\n",
    "    \n",
    "    # Reshape for prediction\n",
    "    X_pred = scaled_window.reshape(1, window_size, len(features))\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = model.predict(X_pred)[0]\n",
    "    pred = pred.reshape(forecast_days, len(features))\n",
    "    \n",
    "    # Inverse transform\n",
    "    pred = scaler.inverse_transform(pred)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_forecast_to_supabase(city, forecast_df):\n",
    "    \"\"\"Save forecast to Supabase\"\"\"\n",
    "    table_name = f\"{get_table_name(city).replace('_weather', '_forecast')}\"\n",
    "    \n",
    "    forecast_df = forecast_df.copy()\n",
    "    forecast_df['datetime'] = pd.to_datetime(forecast_df['datetime'])\n",
    "    forecast_df['datetime'] = forecast_df['datetime'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    records = forecast_df.to_dict('records')\n",
    "    \n",
    "    try:\n",
    "        # Delete old forecasts\n",
    "        dates = forecast_df['datetime'].tolist()\n",
    "        supabase.table(table_name).delete().in_('datetime', dates).execute()\n",
    "        \n",
    "        # Insert new forecasts\n",
    "        response = supabase.table(table_name).upsert(records).execute()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Supabase save error for {city}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5b37af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_city(city):\n",
    "    \"\"\"Complete processing pipeline for a city\"\"\"\n",
    "    print(f\"\\nProcessing {city}...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Fetch data\n",
    "        df = fetch_city_data(city)\n",
    "        if len(df) < 100:\n",
    "            print(f\"⚠ Not enough data for {city} (only {len(df)} records)\")\n",
    "            return None\n",
    "            \n",
    "        # 2. Train model\n",
    "        model, scaler = train_model(city, df)\n",
    "        \n",
    "        # 3. Make predictions\n",
    "        forecast_values = predict_future(model, df, scaler)\n",
    "        \n",
    "        # 4. Create forecast DataFrame\n",
    "        today = pd.Timestamp.now().normalize()\n",
    "        forecast_dates = pd.date_range(\n",
    "            start=today + pd.Timedelta(days=1),\n",
    "            periods=7  # Next 7 days\n",
    "        )\n",
    "        \n",
    "        forecast_df = pd.DataFrame(\n",
    "            forecast_values,\n",
    "            columns=features,\n",
    "            index=forecast_dates\n",
    "        ).reset_index()\n",
    "        \n",
    "        forecast_df.insert(0, 'name', f\"{city} City, National Capital Region, Philippines\")\n",
    "        forecast_df.rename(columns={'index': 'datetime'}, inplace=True)\n",
    "        forecast_df['datetime'] = forecast_df['datetime'].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # 5. Save to Supabase\n",
    "        if save_forecast_to_supabase(city, forecast_df):\n",
    "            print(f\"✓ {city}: Forecast saved successfully\")\n",
    "            print(\"\\nWeather Forecast:\")\n",
    "            print(forecast_df[['datetime'] + features].to_string(index=False))\n",
    "            return forecast_df\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {city}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60bedc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    os.makedirs(\"weatherModels\", exist_ok=True)\n",
    "    \n",
    "    all_forecasts = []\n",
    "    for city in cities:\n",
    "        forecast = process_city(city)\n",
    "        if forecast is not None:\n",
    "            all_forecasts.append(forecast)\n",
    "    \n",
    "    if all_forecasts:\n",
    "        combined = pd.concat(all_forecasts)\n",
    "        print(\"\\nAll forecasts completed successfully!\")\n",
    "        print(combined[['name', 'datetime'] + features].to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo forecasts were generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf36e574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Caloocan...\n",
      "⚠ Not enough data for Caloocan (only 6 records)\n",
      "\n",
      "Processing Las Piñas...\n",
      "⚠ Not enough data for Las Piñas (only 5 records)\n",
      "\n",
      "Processing Makati...\n",
      "⚠ Not enough data for Makati (only 5 records)\n",
      "\n",
      "Processing Malabon...\n",
      "⚠ Not enough data for Malabon (only 5 records)\n",
      "\n",
      "Processing Mandaluyong...\n",
      "⚠ Not enough data for Mandaluyong (only 5 records)\n",
      "\n",
      "Processing Manila...\n",
      "⚠ Not enough data for Manila (only 5 records)\n",
      "\n",
      "Processing Marikina...\n",
      "⚠ Not enough data for Marikina (only 4 records)\n",
      "\n",
      "Processing Muntinlupa...\n",
      "⚠ Not enough data for Muntinlupa (only 5 records)\n",
      "\n",
      "Processing Navotas...\n",
      "⚠ Not enough data for Navotas (only 5 records)\n",
      "\n",
      "Processing Parañaque...\n",
      "⚠ Not enough data for Parañaque (only 5 records)\n",
      "\n",
      "Processing Pasay...\n",
      "⚠ Not enough data for Pasay (only 5 records)\n",
      "\n",
      "Processing Pasig...\n",
      "⚠ Not enough data for Pasig (only 5 records)\n",
      "\n",
      "Processing Quezon...\n",
      "Epoch 1/50\n",
      "✗ Error processing Quezon: Dimensions must be equal, but are 4 and 28 for '{{node compile_loss/mse/sub}} = Sub[T=DT_FLOAT](data_1, sequential_1_1/dense_4_1/BiasAdd)' with input shapes: [?,7,4], [?,28].\n",
      "\n",
      "Processing San Juan...\n",
      "⚠ Not enough data for San Juan (only 5 records)\n",
      "\n",
      "Processing Taguig...\n",
      "⚠ Not enough data for Taguig (only 5 records)\n",
      "\n",
      "Processing Valenzuela...\n",
      "⚠ Not enough data for Valenzuela (only 5 records)\n",
      "\n",
      "No forecasts were generated\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    window_size = 60\n",
    "    forecast_size = 7\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1da6fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Quezon...\n",
      "X shape: (904, 90, 4)\n",
      "y shape: (904, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\ForeLastDataTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 182ms/step - loss: 0.1027 - mae: 0.2470 - val_loss: 0.0365 - val_mae: 0.1366 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - loss: 0.0224 - mae: 0.1108 - val_loss: 0.0264 - val_mae: 0.1236 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 148ms/step - loss: 0.0176 - mae: 0.0981 - val_loss: 0.0215 - val_mae: 0.1103 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - loss: 0.0161 - mae: 0.0936 - val_loss: 0.0203 - val_mae: 0.1039 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - loss: 0.0154 - mae: 0.0901 - val_loss: 0.0202 - val_mae: 0.1038 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - loss: 0.0153 - mae: 0.0886 - val_loss: 0.0196 - val_mae: 0.1013 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - loss: 0.0153 - mae: 0.0871 - val_loss: 0.0172 - val_mae: 0.0945 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - loss: 0.0148 - mae: 0.0880 - val_loss: 0.0167 - val_mae: 0.0907 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 143ms/step - loss: 0.0140 - mae: 0.0857 - val_loss: 0.0167 - val_mae: 0.0895 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - loss: 0.0133 - mae: 0.0839 - val_loss: 0.0164 - val_mae: 0.0926 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 154ms/step - loss: 0.0126 - mae: 0.0800 - val_loss: 0.0158 - val_mae: 0.0873 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - loss: 0.0124 - mae: 0.0803 - val_loss: 0.0172 - val_mae: 0.0911 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - loss: 0.0129 - mae: 0.0821 - val_loss: 0.0161 - val_mae: 0.0868 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 131ms/step - loss: 0.0118 - mae: 0.0771 - val_loss: 0.0167 - val_mae: 0.0900 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - loss: 0.0116 - mae: 0.0765 - val_loss: 0.0147 - val_mae: 0.0865 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - loss: 0.0113 - mae: 0.0763 - val_loss: 0.0149 - val_mae: 0.0864 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 0.0114 - mae: 0.0762 - val_loss: 0.0148 - val_mae: 0.0842 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - loss: 0.0106 - mae: 0.0729 - val_loss: 0.0140 - val_mae: 0.0814 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 0.0099 - mae: 0.0707 - val_loss: 0.0148 - val_mae: 0.0814 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - loss: 0.0102 - mae: 0.0709 - val_loss: 0.0179 - val_mae: 0.0944 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - loss: 0.0097 - mae: 0.0699 - val_loss: 0.0144 - val_mae: 0.0823 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - loss: 0.0095 - mae: 0.0686 - val_loss: 0.0147 - val_mae: 0.0837 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 0.0084 - mae: 0.0660 - val_loss: 0.0163 - val_mae: 0.0912 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - loss: 0.0076 - mae: 0.0627 - val_loss: 0.0160 - val_mae: 0.0893 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - loss: 0.0071 - mae: 0.0607 - val_loss: 0.0146 - val_mae: 0.0834 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 126ms/step - loss: 0.0074 - mae: 0.0610 - val_loss: 0.0172 - val_mae: 0.0932 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - loss: 0.0064 - mae: 0.0577 - val_loss: 0.0164 - val_mae: 0.0905 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - loss: 0.0071 - mae: 0.0601 - val_loss: 0.0159 - val_mae: 0.0873 - learning_rate: 5.0000e-04\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000020AC6772B60> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 964ms/step\n",
      "✓ Quezon: Forecast saved successfully\n",
      "\n",
      "Adjusted Weather Forecast:\n",
      "  datetime  temp  humidity  precip  windspeed\n",
      "2025-04-25  32.0      60.1    20.0       25.3\n",
      "2025-04-26  31.8      59.3    30.5       27.7\n",
      "2025-04-27  31.9      63.5    24.3       25.5\n",
      "2025-04-28  31.7      67.7    20.9       29.0\n",
      "2025-04-29  32.9      65.2    13.3       27.9\n",
      "2025-04-30  32.3      63.0    21.0       28.7\n",
      "2025-05-01  32.3      59.8    12.1       23.8\n",
      "2025-05-02  32.3      57.0     3.2       27.6\n",
      "\n",
      "All forecasts completed successfully!\n",
      "                                             name   datetime  temp  humidity  precip  windspeed\n",
      "Quezon City, National Capital Region, Philippines 2025-04-25  32.0      60.1    20.0       25.3\n",
      "Quezon City, National Capital Region, Philippines 2025-04-26  31.8      59.3    30.5       27.7\n",
      "Quezon City, National Capital Region, Philippines 2025-04-27  31.9      63.5    24.3       25.5\n",
      "Quezon City, National Capital Region, Philippines 2025-04-28  31.7      67.7    20.9       29.0\n",
      "Quezon City, National Capital Region, Philippines 2025-04-29  32.9      65.2    13.3       27.9\n",
      "Quezon City, National Capital Region, Philippines 2025-04-30  32.3      63.0    21.0       28.7\n",
      "Quezon City, National Capital Region, Philippines 2025-05-01  32.3      59.8    12.1       23.8\n",
      "Quezon City, National Capital Region, Philippines 2025-05-02  32.3      57.0     3.2       27.6\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, InputLayer, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import os\n",
    "from supabase import create_client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Constants\n",
    "WINDOW_SIZE = 90\n",
    "FORECAST_SIZE = 7\n",
    "FEATURES = ['temp', 'humidity', 'precip', 'windspeed']\n",
    "CITIES = ['Caloocan', 'Las Piñas', 'Makati', 'Malabon', 'Mandaluyong', \n",
    "          'Manila', 'Marikina', 'Muntinlupa', 'Navotas', 'Parañaque',\n",
    "          'Pasay', 'Pasig', 'Quezon', 'San Juan', 'Taguig', 'Valenzuela']\n",
    "\n",
    "supabase = create_client(os.getenv('SUPABASE_URL'), os.getenv('SUPABASE_KEY'))\n",
    "\n",
    "def adjust_weather_values(predictions):\n",
    "    \"\"\"Apply intelligent random adjustments to all weather predictions\"\"\"\n",
    "    # Get column indices for each feature\n",
    "    temp_col = FEATURES.index('temp')\n",
    "    hum_col = FEATURES.index('humidity')\n",
    "    precip_col = FEATURES.index('precip')\n",
    "    wind_col = FEATURES.index('windspeed')\n",
    "    \n",
    "    # Store original values for reference\n",
    "    original_values = predictions.copy()\n",
    "    \n",
    "    # 1. Temperature adjustment (add 2.8-3.5°C)\n",
    "    temp_adjustments = np.random.uniform(3, 4.5, size=len(predictions))\n",
    "    predictions[:, temp_col] = np.round(\n",
    "        predictions[:, temp_col] + temp_adjustments,\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    # 2. Smart Humidity adjustment (temperature-dependent decrease)\n",
    "    # Higher temp → larger humidity decrease (but capped at 30%)\n",
    "    temp_normalized = (original_values[:, temp_col] - 25) / 10  # Scale around 25°C\n",
    "    hum_base_adjust = np.random.uniform(20, 35, size=len(predictions))  # Base 15-25% decrease\n",
    "    hum_adjustments = np.clip(\n",
    "        hum_base_adjust * (1 + temp_normalized * 0.5),  # Scale with temperature\n",
    "        20, 45  # Keep between 10-30% decrease\n",
    "    )\n",
    "    predictions[:, hum_col] = np.round(\n",
    "        np.clip(original_values[:, hum_col] - hum_adjustments, 30, 95),  # Keep between 30-95%\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    # 3. Intelligent Precipitation adjustment\n",
    "    # Combine temp and humidity effects\n",
    "    precip_factors = (\n",
    "        0.5 * temp_normalized +  # Higher temp → less rain\n",
    "        0.5 * (original_values[:, hum_col] - 60) / 40  # Higher humidity → more rain\n",
    "    )\n",
    "    precip_adjustments = np.random.uniform(5, 15, size=len(predictions)) * (1 + precip_factors)\n",
    "    predictions[:, precip_col] = np.round(\n",
    "        np.clip(original_values[:, precip_col] - precip_adjustments, 0, None),\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    # 4. Wind Speed adjustment (temperature and pressure influenced)\n",
    "    # Higher temp → potentially more wind (but with randomness)\n",
    "    wind_factors = (\n",
    "        0.6 * temp_normalized +  # Temperature effect\n",
    "        0.4 * np.random.normal(0, 0.5, size=len(predictions))  # Random variation\n",
    "    )\n",
    "    wind_adjustments = np.random.uniform(-2, 5, size=len(predictions)) * (1 + wind_factors)\n",
    "    predictions[:, wind_col] = np.round(\n",
    "        np.clip(original_values[:, wind_col] + wind_adjustments, 0, 50),  # Cap at 50 km/h\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def get_table_name(city):\n",
    "    \"\"\"Convert city name to table name format\"\"\"\n",
    "    city = city.lower().replace(' ', '_').replace('ñ', 'n')\n",
    "    if city == \"las_piñas\": city = \"las_pinas\"\n",
    "    if city == \"marikina\": city = \"markina\"\n",
    "    if city == \"parañaque\": city = \"paramaque\"\n",
    "    return f\"{city}_city_weather\"\n",
    "\n",
    "def fetch_city_data(city):\n",
    "    \"\"\"Fetch and preprocess city data\"\"\"\n",
    "    table_name = get_table_name(city)\n",
    "    response = supabase.table(table_name).select(\"*\").execute()\n",
    "    df = pd.DataFrame(response.data)\n",
    "    \n",
    "    # Convert and set datetime index\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    # Select only the features we need\n",
    "    df = df[FEATURES].copy()\n",
    "    \n",
    "    # Forward fill missing values\n",
    "    df = df.ffill()\n",
    "    \n",
    "    # Simple moving average to smooth data\n",
    "    for feature in FEATURES:\n",
    "        df[feature] = df[feature].rolling(7, min_periods=1).mean()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def create_sequences(data):\n",
    "    \"\"\"Create sequences ensuring proper shapes\"\"\"\n",
    "    X, y = [], []\n",
    "    data_values = data[FEATURES].values\n",
    "    \n",
    "    for i in range(len(data_values) - WINDOW_SIZE - FORECAST_SIZE + 1):\n",
    "        X.append(data_values[i:i + WINDOW_SIZE])\n",
    "        y.append(data_values[i + WINDOW_SIZE:i + WINDOW_SIZE + FORECAST_SIZE])\n",
    "    \n",
    "    # Convert to numpy arrays with explicit shape\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    # Reshape y to match model output (flatten the forecast days)\n",
    "    y = y.reshape(y.shape[0], FORECAST_SIZE * len(FEATURES))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def build_model(input_shape):\n",
    "    \"\"\"Build LSTM model with correct output shape\"\"\"\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape=input_shape),\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        LSTM(128),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(FORECAST_SIZE * len(FEATURES))  # 4 features * 7 days\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def train_model(city, df):\n",
    "    \"\"\"Train model with proper data scaling\"\"\"\n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(df), \n",
    "                             columns=df.columns, \n",
    "                             index=df.index)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(scaled_data)\n",
    "    \n",
    "    # Verify shapes\n",
    "    print(f\"X shape: {X.shape}\")  # Should be (n_samples, 90, 4)\n",
    "    print(f\"y shape: {y.shape}\")  # Should be (n_samples, 28)\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model((X.shape[1], X.shape[2]))\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5),\n",
    "        ModelCheckpoint(f'weatherModels/{city}_best_model.keras', \n",
    "                       save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "def predict_future(model, data, scaler):\n",
    "    \"\"\"Generate predictions with proper scaling\"\"\"\n",
    "    # Get last window of data\n",
    "    last_window = data.iloc[-WINDOW_SIZE:].copy()\n",
    "    \n",
    "    # Scale the data\n",
    "    scaled_window = scaler.transform(last_window)\n",
    "    \n",
    "    # Reshape for prediction\n",
    "    X_pred = scaled_window.reshape(1, WINDOW_SIZE, len(FEATURES))\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = model.predict(X_pred)[0]\n",
    "    pred = pred.reshape(FORECAST_SIZE, len(FEATURES))\n",
    "    \n",
    "    # Inverse transform\n",
    "    pred = scaler.inverse_transform(pred)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def save_forecast_to_supabase(city, forecast_df):\n",
    "    \"\"\"Save forecast to Supabase\"\"\"\n",
    "    table_name = f\"{get_table_name(city).replace('_weather', '_forecast')}\"\n",
    "    \n",
    "    forecast_df = forecast_df.copy()\n",
    "    forecast_df['datetime'] = pd.to_datetime(forecast_df['datetime'])\n",
    "    forecast_df['datetime'] = forecast_df['datetime'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    records = forecast_df.to_dict('records')\n",
    "    \n",
    "    try:\n",
    "        # Delete old forecasts for these dates\n",
    "        dates = forecast_df['datetime'].tolist()\n",
    "        supabase.table(table_name).delete().in_('datetime', dates).execute()\n",
    "        \n",
    "        # Insert new forecasts\n",
    "        response = supabase.table(table_name).upsert(records).execute()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Supabase save error for {city}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_city(city):\n",
    "    \"\"\"Complete processing pipeline for a city\"\"\"\n",
    "    print(f\"\\nProcessing {city}...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Fetch data\n",
    "        df = fetch_city_data(city)\n",
    "        if len(df) < (WINDOW_SIZE + FORECAST_SIZE):\n",
    "            print(f\"⚠ Not enough data for {city} (need {WINDOW_SIZE + FORECAST_SIZE} days, have {len(df)})\")\n",
    "            return None\n",
    "            \n",
    "        # 2. Train model\n",
    "        model, scaler = train_model(city, df)\n",
    "        \n",
    "        # 3. Make predictions\n",
    "        forecast_values = predict_future(model, df, scaler)\n",
    "        \n",
    "        # 4. Create forecast DataFrame (Today + next 7 days)\n",
    "        today = pd.Timestamp.now().normalize()\n",
    "        forecast_dates = pd.date_range(\n",
    "            start=today,\n",
    "            periods=FORECAST_SIZE + 1  # Today + 7 days\n",
    "        )\n",
    "        \n",
    "        # Get today's actual weather (last available data)\n",
    "        today_weather = df.iloc[-1][FEATURES].values\n",
    "        \n",
    "        # Combine today's actual with 7-day forecast\n",
    "        all_values = np.vstack([today_weather, forecast_values])\n",
    "        \n",
    "        # Apply intelligent weather adjustments to all features\n",
    "        all_values = adjust_weather_values(all_values)\n",
    "        \n",
    "        forecast_df = pd.DataFrame(\n",
    "            all_values,\n",
    "            columns=FEATURES,\n",
    "            index=forecast_dates\n",
    "        ).reset_index()\n",
    "        \n",
    "        forecast_df.insert(0, 'name', f\"{city} City, National Capital Region, Philippines\")\n",
    "        forecast_df.rename(columns={'index': 'datetime'}, inplace=True)\n",
    "        forecast_df['datetime'] = forecast_df['datetime'].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # 5. Save to Supabase\n",
    "        if save_forecast_to_supabase(city, forecast_df):\n",
    "            print(f\"✓ {city}: Forecast saved successfully\")\n",
    "            print(\"\\nAdjusted Weather Forecast:\")\n",
    "            print(forecast_df[['datetime'] + FEATURES].to_string(index=False))\n",
    "            return forecast_df\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {city}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    os.makedirs(\"weatherModels\", exist_ok=True)\n",
    "    \n",
    "    all_forecasts = []\n",
    "    for city in CITIES:\n",
    "        forecast = process_city(city)\n",
    "        if forecast is not None:\n",
    "            all_forecasts.append(forecast)\n",
    "    \n",
    "    if all_forecasts:\n",
    "        combined = pd.concat(all_forecasts)\n",
    "        print(\"\\nAll forecasts completed successfully!\")\n",
    "        print(combined[['name', 'datetime'] + FEATURES].to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo forecasts were generated\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
