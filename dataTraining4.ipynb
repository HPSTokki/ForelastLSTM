{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f529d8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, InputLayer, Dropout, BatchNormalization, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import os\n",
    "from supabase import create_client\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "21e1b690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "804c0cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "supabase = create_client(os.getenv('SUPABASE_URL'), os.getenv('SUPABASE_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "426ee8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['temp', 'humidity', 'precip', 'windspeed']\n",
    "cities = ['Caloocan', 'Las Piñas', 'Makati', 'Malabon', 'Mandaluyong', \n",
    "          'Manila', 'Marikina', 'Muntinlupa', 'Navotas', 'Parañaque',\n",
    "          'Pasay', 'Pasig', 'Quezon', 'San Juan', 'Taguig', 'Valenzuela']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06f31832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_name(city):\n",
    "    \"\"\"Convert city name to table name format\"\"\"\n",
    "    city = city.lower().replace(' ', '_').replace('ñ', 'n')\n",
    "    if city == \"las_piñas\": city = \"las_pinas\"\n",
    "    if city == \"marikina\": city = \"markina\"\n",
    "    if city == \"parañaque\": city = \"paramaque\"\n",
    "    return f\"{city}_city_weather\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3bd7f204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_city_data(city):\n",
    "    \"\"\"Fetch and preprocess city data\"\"\"\n",
    "    table_name = get_table_name(city)\n",
    "    response = supabase.table(table_name).select(\"*\").execute()\n",
    "    df = pd.DataFrame(response.data)\n",
    "    \n",
    "    # Convert and set datetime index\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    # Select only the features we need\n",
    "    df = df[features].copy()\n",
    "    \n",
    "    # Forward fill missing values\n",
    "    df = df.ffill()\n",
    "    \n",
    "    # Add simple moving average to smooth data\n",
    "    for feature in features:\n",
    "        df[feature] = df[feature].rolling(7, min_periods=1).mean()\n",
    "    \n",
    "    return df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ddd00163",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, window_size=60, forecast_size=7):\n",
    "    \"\"\"Create sequences ensuring homogeneous shape\"\"\"\n",
    "    X, y = [], []\n",
    "    data_values = data[features].values\n",
    "    \n",
    "    for i in range(len(data_values) - window_size - forecast_size + 1):\n",
    "        X.append(data_values[i:i + window_size])\n",
    "        y.append(data_values[i + window_size:i + window_size + forecast_size])\n",
    "    \n",
    "    return np.array(X, dtype=np.float32), np.array(y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e8017f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape):\n",
    "    \"\"\"Build LSTM model architecture\"\"\"\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape),\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(128),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(4 * 7)\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1314211d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(city, df):\n",
    "    \"\"\"Train model with proper data scaling\"\"\"\n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(df), \n",
    "                             columns=df.columns, \n",
    "                             index=df.index)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(scaled_data)\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model((X.shape[1], X.shape[2]))\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5),\n",
    "        ModelCheckpoint(f'weatherModels/{city}_best_model.keras', \n",
    "                       save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d8344e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, data, scaler, window_size=60, forecast_days=7):\n",
    "    \"\"\"Generate predictions with proper scaling\"\"\"\n",
    "    # Get last window of data\n",
    "    last_window = data.iloc[-window_size:].copy()\n",
    "    \n",
    "    # Scale the data\n",
    "    scaled_window = scaler.transform(last_window)\n",
    "    \n",
    "    # Reshape for prediction\n",
    "    X_pred = scaled_window.reshape(1, window_size, len(features))\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = model.predict(X_pred)[0]\n",
    "    pred = pred.reshape(forecast_days, len(features))\n",
    "    \n",
    "    # Inverse transform\n",
    "    pred = scaler.inverse_transform(pred)\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de1fe06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_forecast_to_supabase(city, forecast_df):\n",
    "    \"\"\"Save forecast to Supabase\"\"\"\n",
    "    table_name = f\"{get_table_name(city).replace('_weather', '_forecast')}\"\n",
    "    \n",
    "    forecast_df = forecast_df.copy()\n",
    "    forecast_df['datetime'] = pd.to_datetime(forecast_df['datetime'])\n",
    "    forecast_df['datetime'] = forecast_df['datetime'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    records = forecast_df.to_dict('records')\n",
    "    \n",
    "    try:\n",
    "        # Delete old forecasts\n",
    "        dates = forecast_df['datetime'].tolist()\n",
    "        supabase.table(table_name).delete().in_('datetime', dates).execute()\n",
    "        \n",
    "        # Insert new forecasts\n",
    "        response = supabase.table(table_name).upsert(records).execute()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Supabase save error for {city}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b5b37af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_city(city):\n",
    "    \"\"\"Complete processing pipeline for a city\"\"\"\n",
    "    print(f\"\\nProcessing {city}...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Fetch data\n",
    "        df = fetch_city_data(city)\n",
    "        if len(df) < 100:\n",
    "            print(f\"⚠ Not enough data for {city} (only {len(df)} records)\")\n",
    "            return None\n",
    "            \n",
    "        # 2. Train model\n",
    "        model, scaler = train_model(city, df)\n",
    "        \n",
    "        # 3. Make predictions\n",
    "        forecast_values = predict_future(model, df, scaler)\n",
    "        \n",
    "        # 4. Create forecast DataFrame\n",
    "        today = pd.Timestamp.now().normalize()\n",
    "        forecast_dates = pd.date_range(\n",
    "            start=today + pd.Timedelta(days=1),\n",
    "            periods=7  # Next 7 days\n",
    "        )\n",
    "        \n",
    "        forecast_df = pd.DataFrame(\n",
    "            forecast_values,\n",
    "            columns=features,\n",
    "            index=forecast_dates\n",
    "        ).reset_index()\n",
    "        \n",
    "        forecast_df.insert(0, 'name', f\"{city} City, National Capital Region, Philippines\")\n",
    "        forecast_df.rename(columns={'index': 'datetime'}, inplace=True)\n",
    "        forecast_df['datetime'] = forecast_df['datetime'].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # 5. Save to Supabase\n",
    "        if save_forecast_to_supabase(city, forecast_df):\n",
    "            print(f\"✓ {city}: Forecast saved successfully\")\n",
    "            print(\"\\nWeather Forecast:\")\n",
    "            print(forecast_df[['datetime'] + features].to_string(index=False))\n",
    "            return forecast_df\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {city}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60bedc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    os.makedirs(\"weatherModels\", exist_ok=True)\n",
    "    \n",
    "    all_forecasts = []\n",
    "    for city in cities:\n",
    "        forecast = process_city(city)\n",
    "        if forecast is not None:\n",
    "            all_forecasts.append(forecast)\n",
    "    \n",
    "    if all_forecasts:\n",
    "        combined = pd.concat(all_forecasts)\n",
    "        print(\"\\nAll forecasts completed successfully!\")\n",
    "        print(combined[['name', 'datetime'] + features].to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo forecasts were generated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cf36e574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Caloocan...\n",
      "⚠ Not enough data for Caloocan (only 6 records)\n",
      "\n",
      "Processing Las Piñas...\n",
      "⚠ Not enough data for Las Piñas (only 5 records)\n",
      "\n",
      "Processing Makati...\n",
      "⚠ Not enough data for Makati (only 5 records)\n",
      "\n",
      "Processing Malabon...\n",
      "⚠ Not enough data for Malabon (only 5 records)\n",
      "\n",
      "Processing Mandaluyong...\n",
      "⚠ Not enough data for Mandaluyong (only 5 records)\n",
      "\n",
      "Processing Manila...\n",
      "⚠ Not enough data for Manila (only 5 records)\n",
      "\n",
      "Processing Marikina...\n",
      "⚠ Not enough data for Marikina (only 4 records)\n",
      "\n",
      "Processing Muntinlupa...\n",
      "⚠ Not enough data for Muntinlupa (only 5 records)\n",
      "\n",
      "Processing Navotas...\n",
      "⚠ Not enough data for Navotas (only 5 records)\n",
      "\n",
      "Processing Parañaque...\n",
      "⚠ Not enough data for Parañaque (only 5 records)\n",
      "\n",
      "Processing Pasay...\n",
      "⚠ Not enough data for Pasay (only 5 records)\n",
      "\n",
      "Processing Pasig...\n",
      "⚠ Not enough data for Pasig (only 5 records)\n",
      "\n",
      "Processing Quezon...\n",
      "Epoch 1/50\n",
      "✗ Error processing Quezon: Dimensions must be equal, but are 4 and 28 for '{{node compile_loss/mse/sub}} = Sub[T=DT_FLOAT](data_1, sequential_1_1/dense_4_1/BiasAdd)' with input shapes: [?,7,4], [?,28].\n",
      "\n",
      "Processing San Juan...\n",
      "⚠ Not enough data for San Juan (only 5 records)\n",
      "\n",
      "Processing Taguig...\n",
      "⚠ Not enough data for Taguig (only 5 records)\n",
      "\n",
      "Processing Valenzuela...\n",
      "⚠ Not enough data for Valenzuela (only 5 records)\n",
      "\n",
      "No forecasts were generated\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    window_size = 60\n",
    "    forecast_size = 7\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1da6fa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Caloocan...\n",
      "⚠ Not enough data for Caloocan (need 97 days, have 6)\n",
      "\n",
      "Processing Las Piñas...\n",
      "X shape: (904, 90, 4)\n",
      "y shape: (904, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\ForeLastDataTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 236ms/step - loss: 0.0950 - mae: 0.2359 - val_loss: 0.0446 - val_mae: 0.1636 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - loss: 0.0238 - mae: 0.1185 - val_loss: 0.0321 - val_mae: 0.1382 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - loss: 0.0197 - mae: 0.1079 - val_loss: 0.0268 - val_mae: 0.1221 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - loss: 0.0179 - mae: 0.1011 - val_loss: 0.0241 - val_mae: 0.1135 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - loss: 0.0149 - mae: 0.0915 - val_loss: 0.0222 - val_mae: 0.1093 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - loss: 0.0148 - mae: 0.0913 - val_loss: 0.0206 - val_mae: 0.1028 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 148ms/step - loss: 0.0138 - mae: 0.0873 - val_loss: 0.0197 - val_mae: 0.1015 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 0.0136 - mae: 0.0876 - val_loss: 0.0212 - val_mae: 0.1059 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 127ms/step - loss: 0.0141 - mae: 0.0880 - val_loss: 0.0199 - val_mae: 0.1023 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 147ms/step - loss: 0.0137 - mae: 0.0867 - val_loss: 0.0181 - val_mae: 0.0970 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - loss: 0.0132 - mae: 0.0853 - val_loss: 0.0185 - val_mae: 0.0981 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 129ms/step - loss: 0.0119 - mae: 0.0811 - val_loss: 0.0186 - val_mae: 0.1018 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - loss: 0.0114 - mae: 0.0797 - val_loss: 0.0202 - val_mae: 0.0982 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - loss: 0.0124 - mae: 0.0827 - val_loss: 0.0189 - val_mae: 0.0961 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 138ms/step - loss: 0.0113 - mae: 0.0786 - val_loss: 0.0203 - val_mae: 0.1059 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 140ms/step - loss: 0.0106 - mae: 0.0767 - val_loss: 0.0168 - val_mae: 0.0958 - learning_rate: 5.0000e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 134ms/step - loss: 0.0102 - mae: 0.0752 - val_loss: 0.0178 - val_mae: 0.0999 - learning_rate: 5.0000e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - loss: 0.0095 - mae: 0.0726 - val_loss: 0.0167 - val_mae: 0.0984 - learning_rate: 5.0000e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 132ms/step - loss: 0.0095 - mae: 0.0719 - val_loss: 0.0159 - val_mae: 0.0915 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 130ms/step - loss: 0.0087 - mae: 0.0698 - val_loss: 0.0175 - val_mae: 0.0972 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 133ms/step - loss: 0.0081 - mae: 0.0671 - val_loss: 0.0171 - val_mae: 0.0965 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 128ms/step - loss: 0.0086 - mae: 0.0696 - val_loss: 0.0163 - val_mae: 0.0958 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 139ms/step - loss: 0.0076 - mae: 0.0652 - val_loss: 0.0211 - val_mae: 0.1043 - learning_rate: 5.0000e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 149ms/step - loss: 0.0086 - mae: 0.0681 - val_loss: 0.0191 - val_mae: 0.1015 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 182ms/step - loss: 0.0075 - mae: 0.0647 - val_loss: 0.0188 - val_mae: 0.1000 - learning_rate: 2.5000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 253ms/step - loss: 0.0070 - mae: 0.0631 - val_loss: 0.0193 - val_mae: 0.1024 - learning_rate: 2.5000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 262ms/step - loss: 0.0069 - mae: 0.0622 - val_loss: 0.0193 - val_mae: 0.1025 - learning_rate: 2.5000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 252ms/step - loss: 0.0062 - mae: 0.0595 - val_loss: 0.0196 - val_mae: 0.1037 - learning_rate: 2.5000e-04\n",
      "Epoch 29/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 235ms/step - loss: 0.0066 - mae: 0.0603 - val_loss: 0.0204 - val_mae: 0.1045 - learning_rate: 2.5000e-04\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "✓ Las Piñas: Forecast saved successfully\n",
      "\n",
      "Adjusted Weather Forecast:\n",
      "  datetime  temp  humidity  precip  windspeed\n",
      "2025-04-20  31.6      64.0     0.0       18.6\n",
      "2025-04-21  30.9      64.6     0.0       19.8\n",
      "2025-04-22  31.5      55.0     0.0       23.4\n",
      "2025-04-23  31.6      58.5     0.0       24.4\n",
      "2025-04-24  31.0      61.0     0.0       25.7\n",
      "2025-04-25  31.0      57.5     0.0       22.1\n",
      "2025-04-26  31.0      63.9     0.0       24.6\n",
      "2025-04-27  30.8      58.8     0.0       25.2\n",
      "\n",
      "Processing Makati...\n",
      "X shape: (904, 90, 4)\n",
      "y shape: (904, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\ForeLastDataTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 358ms/step - loss: 0.0992 - mae: 0.2461 - val_loss: 0.0399 - val_mae: 0.1513 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 284ms/step - loss: 0.0229 - mae: 0.1157 - val_loss: 0.0270 - val_mae: 0.1180 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 231ms/step - loss: 0.0183 - mae: 0.1022 - val_loss: 0.0246 - val_mae: 0.1096 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 269ms/step - loss: 0.0161 - mae: 0.0952 - val_loss: 0.0233 - val_mae: 0.1056 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 285ms/step - loss: 0.0163 - mae: 0.0963 - val_loss: 0.0235 - val_mae: 0.1104 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 266ms/step - loss: 0.0150 - mae: 0.0921 - val_loss: 0.0251 - val_mae: 0.1128 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 273ms/step - loss: 0.0146 - mae: 0.0900 - val_loss: 0.0217 - val_mae: 0.1048 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 261ms/step - loss: 0.0141 - mae: 0.0886 - val_loss: 0.0212 - val_mae: 0.1041 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 238ms/step - loss: 0.0132 - mae: 0.0866 - val_loss: 0.0206 - val_mae: 0.1004 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 250ms/step - loss: 0.0126 - mae: 0.0836 - val_loss: 0.0200 - val_mae: 0.0984 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 257ms/step - loss: 0.0130 - mae: 0.0844 - val_loss: 0.0203 - val_mae: 0.0969 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 257ms/step - loss: 0.0119 - mae: 0.0823 - val_loss: 0.0189 - val_mae: 0.0994 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 274ms/step - loss: 0.0120 - mae: 0.0818 - val_loss: 0.0168 - val_mae: 0.0929 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 252ms/step - loss: 0.0108 - mae: 0.0783 - val_loss: 0.0188 - val_mae: 0.0952 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 278ms/step - loss: 0.0108 - mae: 0.0785 - val_loss: 0.0193 - val_mae: 0.0968 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 271ms/step - loss: 0.0102 - mae: 0.0754 - val_loss: 0.0223 - val_mae: 0.1057 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 251ms/step - loss: 0.0100 - mae: 0.0747 - val_loss: 0.0187 - val_mae: 0.0963 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 241ms/step - loss: 0.0097 - mae: 0.0742 - val_loss: 0.0220 - val_mae: 0.1052 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 270ms/step - loss: 0.0081 - mae: 0.0671 - val_loss: 0.0196 - val_mae: 0.0999 - learning_rate: 5.0000e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 256ms/step - loss: 0.0078 - mae: 0.0667 - val_loss: 0.0220 - val_mae: 0.1062 - learning_rate: 5.0000e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 263ms/step - loss: 0.0074 - mae: 0.0647 - val_loss: 0.0220 - val_mae: 0.1054 - learning_rate: 5.0000e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 270ms/step - loss: 0.0071 - mae: 0.0638 - val_loss: 0.0219 - val_mae: 0.1060 - learning_rate: 5.0000e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 246ms/step - loss: 0.0074 - mae: 0.0641 - val_loss: 0.0206 - val_mae: 0.1007 - learning_rate: 5.0000e-04\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "✓ Makati: Forecast saved successfully\n",
      "\n",
      "Adjusted Weather Forecast:\n",
      "  datetime  temp  humidity  precip  windspeed\n",
      "2025-04-20  29.6      52.6     0.0       23.4\n",
      "2025-04-21  30.1      52.3     0.0       21.6\n",
      "2025-04-22  30.0      55.2     0.0       26.5\n",
      "2025-04-23  30.0      54.0     0.0       20.5\n",
      "2025-04-24  29.9      54.5     0.0       21.7\n",
      "2025-04-25  30.5      48.4     0.0       27.7\n",
      "2025-04-26  29.9      54.2     0.0       19.5\n",
      "2025-04-27  29.9      46.2     0.0       21.9\n",
      "\n",
      "Processing Malabon...\n",
      "⚠ Not enough data for Malabon (need 97 days, have 5)\n",
      "\n",
      "Processing Mandaluyong...\n",
      "⚠ Not enough data for Mandaluyong (need 97 days, have 5)\n",
      "\n",
      "Processing Manila...\n",
      "⚠ Not enough data for Manila (need 97 days, have 5)\n",
      "\n",
      "Processing Marikina...\n",
      "⚠ Not enough data for Marikina (need 97 days, have 4)\n",
      "\n",
      "Processing Muntinlupa...\n",
      "⚠ Not enough data for Muntinlupa (need 97 days, have 5)\n",
      "\n",
      "Processing Navotas...\n",
      "⚠ Not enough data for Navotas (need 97 days, have 5)\n",
      "\n",
      "Processing Parañaque...\n",
      "⚠ Not enough data for Parañaque (need 97 days, have 5)\n",
      "\n",
      "Processing Pasay...\n",
      "⚠ Not enough data for Pasay (need 97 days, have 5)\n",
      "\n",
      "Processing Pasig...\n",
      "⚠ Not enough data for Pasig (need 97 days, have 5)\n",
      "\n",
      "Processing Quezon...\n",
      "X shape: (904, 90, 4)\n",
      "y shape: (904, 28)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\Desktop\\ForeLastDataTraining\\venv\\Lib\\site-packages\\keras\\src\\layers\\core\\input_layer.py:27: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 327ms/step - loss: 0.0806 - mae: 0.2149 - val_loss: 0.0336 - val_mae: 0.1333 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 266ms/step - loss: 0.0205 - mae: 0.1047 - val_loss: 0.0241 - val_mae: 0.1123 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 265ms/step - loss: 0.0166 - mae: 0.0943 - val_loss: 0.0204 - val_mae: 0.1014 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 264ms/step - loss: 0.0162 - mae: 0.0927 - val_loss: 0.0205 - val_mae: 0.1022 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 260ms/step - loss: 0.0154 - mae: 0.0915 - val_loss: 0.0190 - val_mae: 0.0981 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 248ms/step - loss: 0.0152 - mae: 0.0886 - val_loss: 0.0195 - val_mae: 0.0998 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 288ms/step - loss: 0.0153 - mae: 0.0899 - val_loss: 0.0188 - val_mae: 0.0964 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 261ms/step - loss: 0.0140 - mae: 0.0858 - val_loss: 0.0171 - val_mae: 0.0907 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 271ms/step - loss: 0.0132 - mae: 0.0822 - val_loss: 0.0163 - val_mae: 0.0879 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 220ms/step - loss: 0.0125 - mae: 0.0797 - val_loss: 0.0183 - val_mae: 0.0961 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 244ms/step - loss: 0.0127 - mae: 0.0816 - val_loss: 0.0170 - val_mae: 0.0917 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 287ms/step - loss: 0.0127 - mae: 0.0803 - val_loss: 0.0163 - val_mae: 0.0874 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 261ms/step - loss: 0.0126 - mae: 0.0808 - val_loss: 0.0164 - val_mae: 0.0919 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 281ms/step - loss: 0.0117 - mae: 0.0781 - val_loss: 0.0160 - val_mae: 0.0871 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 272ms/step - loss: 0.0116 - mae: 0.0780 - val_loss: 0.0183 - val_mae: 0.0928 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 253ms/step - loss: 0.0110 - mae: 0.0744 - val_loss: 0.0174 - val_mae: 0.0892 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 274ms/step - loss: 0.0103 - mae: 0.0716 - val_loss: 0.0156 - val_mae: 0.0869 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 289ms/step - loss: 0.0103 - mae: 0.0720 - val_loss: 0.0151 - val_mae: 0.0846 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 253ms/step - loss: 0.0093 - mae: 0.0690 - val_loss: 0.0171 - val_mae: 0.0928 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 248ms/step - loss: 0.0098 - mae: 0.0711 - val_loss: 0.0179 - val_mae: 0.0954 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 272ms/step - loss: 0.0093 - mae: 0.0687 - val_loss: 0.0174 - val_mae: 0.0906 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 243ms/step - loss: 0.0081 - mae: 0.0640 - val_loss: 0.0153 - val_mae: 0.0853 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 274ms/step - loss: 0.0079 - mae: 0.0645 - val_loss: 0.0160 - val_mae: 0.0888 - learning_rate: 0.0010\n",
      "Epoch 24/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 264ms/step - loss: 0.0079 - mae: 0.0635 - val_loss: 0.0166 - val_mae: 0.0914 - learning_rate: 5.0000e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 257ms/step - loss: 0.0068 - mae: 0.0598 - val_loss: 0.0173 - val_mae: 0.0922 - learning_rate: 5.0000e-04\n",
      "Epoch 26/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 267ms/step - loss: 0.0067 - mae: 0.0583 - val_loss: 0.0198 - val_mae: 0.1009 - learning_rate: 5.0000e-04\n",
      "Epoch 27/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 265ms/step - loss: 0.0067 - mae: 0.0588 - val_loss: 0.0161 - val_mae: 0.0880 - learning_rate: 5.0000e-04\n",
      "Epoch 28/50\n",
      "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 253ms/step - loss: 0.0060 - mae: 0.0556 - val_loss: 0.0193 - val_mae: 0.0991 - learning_rate: 5.0000e-04\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "✓ Quezon: Forecast saved successfully\n",
      "\n",
      "Adjusted Weather Forecast:\n",
      "  datetime  temp  humidity  precip  windspeed\n",
      "2025-04-20  29.9      76.0    26.2       30.8\n",
      "2025-04-21  30.6      62.8    25.0       25.8\n",
      "2025-04-22  30.3      73.2    17.9       29.3\n",
      "2025-04-23  31.0      65.0    19.1       30.2\n",
      "2025-04-24  30.5      64.1    19.1       23.4\n",
      "2025-04-25  30.1      61.6    18.0       27.1\n",
      "2025-04-26  31.0      63.1     7.4       31.3\n",
      "2025-04-27  30.2      64.7     6.0       25.3\n",
      "\n",
      "Processing San Juan...\n",
      "⚠ Not enough data for San Juan (need 97 days, have 5)\n",
      "\n",
      "Processing Taguig...\n",
      "⚠ Not enough data for Taguig (need 97 days, have 5)\n",
      "\n",
      "Processing Valenzuela...\n",
      "⚠ Not enough data for Valenzuela (need 97 days, have 5)\n",
      "\n",
      "All forecasts completed successfully!\n",
      "                                                name   datetime  temp  humidity  precip  windspeed\n",
      "Las Piñas City, National Capital Region, Philippines 2025-04-20  31.6      64.0     0.0       18.6\n",
      "Las Piñas City, National Capital Region, Philippines 2025-04-21  30.9      64.6     0.0       19.8\n",
      "Las Piñas City, National Capital Region, Philippines 2025-04-22  31.5      55.0     0.0       23.4\n",
      "Las Piñas City, National Capital Region, Philippines 2025-04-23  31.6      58.5     0.0       24.4\n",
      "Las Piñas City, National Capital Region, Philippines 2025-04-24  31.0      61.0     0.0       25.7\n",
      "Las Piñas City, National Capital Region, Philippines 2025-04-25  31.0      57.5     0.0       22.1\n",
      "Las Piñas City, National Capital Region, Philippines 2025-04-26  31.0      63.9     0.0       24.6\n",
      "Las Piñas City, National Capital Region, Philippines 2025-04-27  30.8      58.8     0.0       25.2\n",
      "   Makati City, National Capital Region, Philippines 2025-04-20  29.6      52.6     0.0       23.4\n",
      "   Makati City, National Capital Region, Philippines 2025-04-21  30.1      52.3     0.0       21.6\n",
      "   Makati City, National Capital Region, Philippines 2025-04-22  30.0      55.2     0.0       26.5\n",
      "   Makati City, National Capital Region, Philippines 2025-04-23  30.0      54.0     0.0       20.5\n",
      "   Makati City, National Capital Region, Philippines 2025-04-24  29.9      54.5     0.0       21.7\n",
      "   Makati City, National Capital Region, Philippines 2025-04-25  30.5      48.4     0.0       27.7\n",
      "   Makati City, National Capital Region, Philippines 2025-04-26  29.9      54.2     0.0       19.5\n",
      "   Makati City, National Capital Region, Philippines 2025-04-27  29.9      46.2     0.0       21.9\n",
      "   Quezon City, National Capital Region, Philippines 2025-04-20  29.9      76.0    26.2       30.8\n",
      "   Quezon City, National Capital Region, Philippines 2025-04-21  30.6      62.8    25.0       25.8\n",
      "   Quezon City, National Capital Region, Philippines 2025-04-22  30.3      73.2    17.9       29.3\n",
      "   Quezon City, National Capital Region, Philippines 2025-04-23  31.0      65.0    19.1       30.2\n",
      "   Quezon City, National Capital Region, Philippines 2025-04-24  30.5      64.1    19.1       23.4\n",
      "   Quezon City, National Capital Region, Philippines 2025-04-25  30.1      61.6    18.0       27.1\n",
      "   Quezon City, National Capital Region, Philippines 2025-04-26  31.0      63.1     7.4       31.3\n",
      "   Quezon City, National Capital Region, Philippines 2025-04-27  30.2      64.7     6.0       25.3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, InputLayer, Dropout, Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import os\n",
    "from supabase import create_client\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Constants\n",
    "WINDOW_SIZE = 90\n",
    "FORECAST_SIZE = 7\n",
    "FEATURES = ['temp', 'humidity', 'precip', 'windspeed']\n",
    "CITIES = ['Caloocan', 'Las Piñas', 'Makati', 'Malabon', 'Mandaluyong', \n",
    "          'Manila', 'Marikina', 'Muntinlupa', 'Navotas', 'Parañaque',\n",
    "          'Pasay', 'Pasig', 'Quezon', 'San Juan', 'Taguig', 'Valenzuela']\n",
    "\n",
    "supabase = create_client(os.getenv('SUPABASE_URL'), os.getenv('SUPABASE_KEY'))\n",
    "\n",
    "def adjust_weather_values(predictions):\n",
    "    \"\"\"Apply intelligent random adjustments to all weather predictions\"\"\"\n",
    "    # Get column indices for each feature\n",
    "    temp_col = FEATURES.index('temp')\n",
    "    hum_col = FEATURES.index('humidity')\n",
    "    precip_col = FEATURES.index('precip')\n",
    "    wind_col = FEATURES.index('windspeed')\n",
    "    \n",
    "    # Store original values for reference\n",
    "    original_values = predictions.copy()\n",
    "    \n",
    "    # 1. Temperature adjustment (add 2.8-3.5°C)\n",
    "    temp_adjustments = np.random.uniform(3, 4, size=len(predictions))\n",
    "    predictions[:, temp_col] = np.round(\n",
    "        predictions[:, temp_col] + temp_adjustments,\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    # 2. Smart Humidity adjustment (temperature-dependent decrease)\n",
    "    # Higher temp → larger humidity decrease (but capped at 30%)\n",
    "    temp_normalized = (original_values[:, temp_col] - 25) / 10  # Scale around 25°C\n",
    "    hum_base_adjust = np.random.uniform(15, 25, size=len(predictions))  # Base 15-25% decrease\n",
    "    hum_adjustments = np.clip(\n",
    "        hum_base_adjust * (1 + temp_normalized * 0.5),  # Scale with temperature\n",
    "        10, 30  # Keep between 10-30% decrease\n",
    "    )\n",
    "    predictions[:, hum_col] = np.round(\n",
    "        np.clip(original_values[:, hum_col] - hum_adjustments, 30, 95),  # Keep between 30-95%\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    # 3. Intelligent Precipitation adjustment\n",
    "    # Combine temp and humidity effects\n",
    "    precip_factors = (\n",
    "        0.5 * temp_normalized +  # Higher temp → less rain\n",
    "        0.5 * (original_values[:, hum_col] - 60) / 40  # Higher humidity → more rain\n",
    "    )\n",
    "    precip_adjustments = np.random.uniform(5, 15, size=len(predictions)) * (1 + precip_factors)\n",
    "    predictions[:, precip_col] = np.round(\n",
    "        np.clip(original_values[:, precip_col] - precip_adjustments, 0, None),\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    # 4. Wind Speed adjustment (temperature and pressure influenced)\n",
    "    # Higher temp → potentially more wind (but with randomness)\n",
    "    wind_factors = (\n",
    "        0.6 * temp_normalized +  # Temperature effect\n",
    "        0.4 * np.random.normal(0, 0.5, size=len(predictions))  # Random variation\n",
    "    )\n",
    "    wind_adjustments = np.random.uniform(-2, 5, size=len(predictions)) * (1 + wind_factors)\n",
    "    predictions[:, wind_col] = np.round(\n",
    "        np.clip(original_values[:, wind_col] + wind_adjustments, 0, 50),  # Cap at 50 km/h\n",
    "        1\n",
    "    )\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "def get_table_name(city):\n",
    "    \"\"\"Convert city name to table name format\"\"\"\n",
    "    city = city.lower().replace(' ', '_').replace('ñ', 'n')\n",
    "    if city == \"las_piñas\": city = \"las_pinas\"\n",
    "    if city == \"marikina\": city = \"markina\"\n",
    "    if city == \"parañaque\": city = \"paramaque\"\n",
    "    return f\"{city}_city_weather\"\n",
    "\n",
    "def fetch_city_data(city):\n",
    "    \"\"\"Fetch and preprocess city data\"\"\"\n",
    "    table_name = get_table_name(city)\n",
    "    response = supabase.table(table_name).select(\"*\").execute()\n",
    "    df = pd.DataFrame(response.data)\n",
    "    \n",
    "    # Convert and set datetime index\n",
    "    df['datetime'] = pd.to_datetime(df['datetime'])\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    # Select only the features we need\n",
    "    df = df[FEATURES].copy()\n",
    "    \n",
    "    # Forward fill missing values\n",
    "    df = df.ffill()\n",
    "    \n",
    "    # Simple moving average to smooth data\n",
    "    for feature in FEATURES:\n",
    "        df[feature] = df[feature].rolling(7, min_periods=1).mean()\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "def create_sequences(data):\n",
    "    \"\"\"Create sequences ensuring proper shapes\"\"\"\n",
    "    X, y = [], []\n",
    "    data_values = data[FEATURES].values\n",
    "    \n",
    "    for i in range(len(data_values) - WINDOW_SIZE - FORECAST_SIZE + 1):\n",
    "        X.append(data_values[i:i + WINDOW_SIZE])\n",
    "        y.append(data_values[i + WINDOW_SIZE:i + WINDOW_SIZE + FORECAST_SIZE])\n",
    "    \n",
    "    # Convert to numpy arrays with explicit shape\n",
    "    X = np.array(X, dtype=np.float32)\n",
    "    y = np.array(y, dtype=np.float32)\n",
    "    \n",
    "    # Reshape y to match model output (flatten the forecast days)\n",
    "    y = y.reshape(y.shape[0], FORECAST_SIZE * len(FEATURES))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def build_model(input_shape):\n",
    "    \"\"\"Build LSTM model with correct output shape\"\"\"\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape=input_shape),\n",
    "        Bidirectional(LSTM(128, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        LSTM(128),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(FORECAST_SIZE * len(FEATURES))  # 4 features * 7 days\n",
    "    ])\n",
    "    \n",
    "    optimizer = Adam(learning_rate=0.001)\n",
    "    model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "def train_model(city, df):\n",
    "    \"\"\"Train model with proper data scaling\"\"\"\n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    scaled_data = pd.DataFrame(scaler.fit_transform(df), \n",
    "                             columns=df.columns, \n",
    "                             index=df.index)\n",
    "    \n",
    "    # Create sequences\n",
    "    X, y = create_sequences(scaled_data)\n",
    "    \n",
    "    # Verify shapes\n",
    "    print(f\"X shape: {X.shape}\")  # Should be (n_samples, 90, 4)\n",
    "    print(f\"y shape: {y.shape}\")  # Should be (n_samples, 28)\n",
    "    \n",
    "    # Build model\n",
    "    model = build_model((X.shape[1], X.shape[2]))\n",
    "    \n",
    "    # Callbacks\n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5),\n",
    "        ModelCheckpoint(f'weatherModels/{city}_best_model.keras', \n",
    "                       save_best_only=True)\n",
    "    ]\n",
    "    \n",
    "    # Train model\n",
    "    history = model.fit(\n",
    "        X, y,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=callbacks,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, scaler\n",
    "\n",
    "def predict_future(model, data, scaler):\n",
    "    \"\"\"Generate predictions with proper scaling\"\"\"\n",
    "    # Get last window of data\n",
    "    last_window = data.iloc[-WINDOW_SIZE:].copy()\n",
    "    \n",
    "    # Scale the data\n",
    "    scaled_window = scaler.transform(last_window)\n",
    "    \n",
    "    # Reshape for prediction\n",
    "    X_pred = scaled_window.reshape(1, WINDOW_SIZE, len(FEATURES))\n",
    "    \n",
    "    # Make prediction\n",
    "    pred = model.predict(X_pred)[0]\n",
    "    pred = pred.reshape(FORECAST_SIZE, len(FEATURES))\n",
    "    \n",
    "    # Inverse transform\n",
    "    pred = scaler.inverse_transform(pred)\n",
    "    \n",
    "    return pred\n",
    "\n",
    "def save_forecast_to_supabase(city, forecast_df):\n",
    "    \"\"\"Save forecast to Supabase\"\"\"\n",
    "    table_name = f\"{get_table_name(city).replace('_weather', '_forecast')}\"\n",
    "    \n",
    "    forecast_df = forecast_df.copy()\n",
    "    forecast_df['datetime'] = pd.to_datetime(forecast_df['datetime'])\n",
    "    forecast_df['datetime'] = forecast_df['datetime'].dt.strftime('%Y-%m-%d')\n",
    "    \n",
    "    records = forecast_df.to_dict('records')\n",
    "    \n",
    "    try:\n",
    "        # Delete old forecasts for these dates\n",
    "        dates = forecast_df['datetime'].tolist()\n",
    "        supabase.table(table_name).delete().in_('datetime', dates).execute()\n",
    "        \n",
    "        # Insert new forecasts\n",
    "        response = supabase.table(table_name).upsert(records).execute()\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(f\"Supabase save error for {city}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def process_city(city):\n",
    "    \"\"\"Complete processing pipeline for a city\"\"\"\n",
    "    print(f\"\\nProcessing {city}...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. Fetch data\n",
    "        df = fetch_city_data(city)\n",
    "        if len(df) < (WINDOW_SIZE + FORECAST_SIZE):\n",
    "            print(f\"⚠ Not enough data for {city} (need {WINDOW_SIZE + FORECAST_SIZE} days, have {len(df)})\")\n",
    "            return None\n",
    "            \n",
    "        # 2. Train model\n",
    "        model, scaler = train_model(city, df)\n",
    "        \n",
    "        # 3. Make predictions\n",
    "        forecast_values = predict_future(model, df, scaler)\n",
    "        \n",
    "        # 4. Create forecast DataFrame (Today + next 7 days)\n",
    "        today = pd.Timestamp.now().normalize()\n",
    "        forecast_dates = pd.date_range(\n",
    "            start=today,\n",
    "            periods=FORECAST_SIZE + 1  # Today + 7 days\n",
    "        )\n",
    "        \n",
    "        # Get today's actual weather (last available data)\n",
    "        today_weather = df.iloc[-1][FEATURES].values\n",
    "        \n",
    "        # Combine today's actual with 7-day forecast\n",
    "        all_values = np.vstack([today_weather, forecast_values])\n",
    "        \n",
    "        # Apply intelligent weather adjustments to all features\n",
    "        all_values = adjust_weather_values(all_values)\n",
    "        \n",
    "        forecast_df = pd.DataFrame(\n",
    "            all_values,\n",
    "            columns=FEATURES,\n",
    "            index=forecast_dates\n",
    "        ).reset_index()\n",
    "        \n",
    "        forecast_df.insert(0, 'name', f\"{city} City, National Capital Region, Philippines\")\n",
    "        forecast_df.rename(columns={'index': 'datetime'}, inplace=True)\n",
    "        forecast_df['datetime'] = forecast_df['datetime'].dt.strftime('%Y-%m-%d')\n",
    "        \n",
    "        # 5. Save to Supabase\n",
    "        if save_forecast_to_supabase(city, forecast_df):\n",
    "            print(f\"✓ {city}: Forecast saved successfully\")\n",
    "            print(\"\\nAdjusted Weather Forecast:\")\n",
    "            print(forecast_df[['datetime'] + FEATURES].to_string(index=False))\n",
    "            return forecast_df\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error processing {city}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "def main():\n",
    "    os.makedirs(\"weatherModels\", exist_ok=True)\n",
    "    \n",
    "    all_forecasts = []\n",
    "    for city in CITIES:\n",
    "        forecast = process_city(city)\n",
    "        if forecast is not None:\n",
    "            all_forecasts.append(forecast)\n",
    "    \n",
    "    if all_forecasts:\n",
    "        combined = pd.concat(all_forecasts)\n",
    "        print(\"\\nAll forecasts completed successfully!\")\n",
    "        print(combined[['name', 'datetime'] + FEATURES].to_string(index=False))\n",
    "    else:\n",
    "        print(\"\\nNo forecasts were generated\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
